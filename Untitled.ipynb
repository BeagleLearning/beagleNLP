{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TextRank implementation for tagging clusters with keyword(s)\n",
    "\"\"\"\n",
    "import regex as re\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import operator\n",
    "import time\n",
    "\n",
    "parts_of_speech = [\"NOUN\", \"PROPN\", \"VERB\"]\n",
    "dampen_const = 0.85\n",
    "min_diff = 1e-5\n",
    "# Potentially dynamic and set to be relative to the average length of sentences(???)\n",
    "window_size = 2\n",
    "\n",
    "# Note: possibly ignore POS when the corpus size is small(???)\n",
    "def process_corpus(corpus, parts_of_speech=parts_of_speech, ignore_POS=False):\n",
    "    \"\"\"\n",
    "    Process corpus to remove stopwords and return lemmatized words\n",
    "    \n",
    "    Parameters:\n",
    "        corpus (list): List of spacy documents\n",
    "        parts_of_speech (list): List of they types of words we want to pay attention to\n",
    "            (default is defined above as parts_of_speech)\n",
    "        ignore_POS (bool): Whether or not to ignore the POS and consider every word\n",
    "    \n",
    "    Returns:\n",
    "        sentences (list): List of processed documents that only contain the parts of speech needed\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for document in corpus:\n",
    "        for sent in document.sents:\n",
    "            words = []\n",
    "            for token in sent:\n",
    "                if ignore_POS:\n",
    "                    if token.is_stop is False and token.is_punct is False:\n",
    "                        words.append(token.lemma_)\n",
    "                else:\n",
    "                    if token.pos_ in parts_of_speech and token.is_stop is False and token.is_punct is False:\n",
    "                        words.append(token.lemma_)\n",
    "            sentences.append(words)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "def get_vocab(sentences):\n",
    "    \"\"\" \n",
    "    Calculates the text frequency of a word in a sentence\n",
    "\n",
    "    Parameters:\n",
    "        sentences (list): A list of strings (lemma_) that make up a given cluster\n",
    "\n",
    "    Returns:\n",
    "        vocabulary (OrderedDict):  collection of keywords and their text frequency\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = OrderedDict()\n",
    "    i = 0\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = i\n",
    "                i += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "\n",
    "def token_pairs(sentences):\n",
    "    \"\"\" \n",
    "    Get the words that are linking to each other\n",
    "\n",
    "    Parameters:\n",
    "        sentences (list): List of spacy documents\n",
    "\n",
    "    Returns:\n",
    "        pairs (list): List of tuples that contain two words that link to one another (word1, word2)\n",
    "    \"\"\"\n",
    "\n",
    "    pairs = list()\n",
    "    for sentence in sentences:\n",
    "        for ix, word in enumerate(sentence):\n",
    "            for jx in range(ix+1, ix+window_size):\n",
    "                if jx >= len(sentence):\n",
    "                    break\n",
    "                pair = (word, sentence[jx])\n",
    "                if pair not in pairs:\n",
    "                    pairs.append(pair)\n",
    "    # print(\"Token pairs\")\n",
    "    # print(pairs)\n",
    "    return pairs\n",
    "\n",
    "\n",
    "\n",
    "def get_matrix(vocab, token_pairs):\n",
    "    \"\"\" \n",
    "    Returns normalized matrix representing the inboud and outboud links of each word\n",
    "\n",
    "    Parameters:\n",
    "        vocab (OrderedDict): collection of words and their frequency\n",
    "\n",
    "    Returns:\n",
    "        normalized matrix (list): A normalized matrix showing the weight (textrank) of each word\n",
    "    \"\"\"\n",
    "\n",
    "    # Build matrix\n",
    "    vocab_size = len(vocab)\n",
    "    g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "    for word1, word2 in token_pairs:\n",
    "        i, j = vocab[word1], vocab[word2]\n",
    "        g[i][j] = 1\n",
    "\n",
    "    g = g + g.T - np.diag(g.diagonal())\n",
    "\n",
    "    # Normalize matrix by column\n",
    "    norm = np.sum(g, axis=0)\n",
    "    g_norm = np.divide(g, norm, where=norm != 0)\n",
    "\n",
    "    return g_norm\n",
    "\n",
    "\n",
    "def remove_stopwords(corpus):\n",
    "    \"\"\" Returs list of tokens excluding stop words \"\"\"\n",
    "    return [token for token in corpus.doc if not token.is_stop]\n",
    "\n",
    "\n",
    "def get_keywords(cluster, corpus, return_one=True, use_idf=False):\n",
    "    \"\"\"\n",
    "    Identifies the keywords in a list of questions using the TextRank algorithm\n",
    "\n",
    "    Parameters:\n",
    "        cluster (list): A list of spacy Doc strings that make up a given cluster\n",
    "        return_one (bool): Flag to determine whether we return only one keyword or not\n",
    "                (default is True)\n",
    "    Returns:\n",
    "        keyword (str): a single keyword if flag `return_one` is set to true,\n",
    "        node_weights (OrderedDict):  collection of keywords and their textrank score\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = process_corpus(cluster)\n",
    "    vocab = get_vocab(sentences)\n",
    "    tokens = token_pairs(sentences)\n",
    "    matrix = get_matrix(vocab, tokens)\n",
    "\n",
    "    init_rank = np.array([1] * len(vocab))\n",
    "    previous_rank = 0\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        init_rank = (1 - dampen_const) + dampen_const * \\\n",
    "            np.dot(matrix, init_rank)\n",
    "        if abs(previous_rank - sum(init_rank)) < min_diff:\n",
    "            break\n",
    "        else:\n",
    "            previous_rank = sum(init_rank)\n",
    "\n",
    "    start = time.time()\n",
    "    node_weight = OrderedDict()\n",
    "    for word, index in vocab.items():\n",
    "        node_weight[word] = init_rank[index]\n",
    "\n",
    "    node_weight = OrderedDict(\n",
    "        sorted(node_weight.items(), key=lambda item: item[1]))\n",
    "\n",
    "\n",
    "    # print(\"\\n Node Weights per cluster: \")\n",
    "    # print(cluster)\n",
    "    # print(node_weight)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    kwords = list(node_weight)\n",
    "    keyword = \"uncategorized\"\n",
    "    if return_one:\n",
    "        if use_idf:\n",
    "            ranks = list(node_weight.values())\n",
    "            trIDFScores = {}\n",
    "            for ix, word in enumerate(kwords):\n",
    "                idfScore = 0.1\n",
    "                try:\n",
    "                    idfScore = corpus.idf_[corpus.vocabulary_[word]]\n",
    "                except:\n",
    "                    pass\n",
    "                trIDFScores[word] = ranks[ix] * (idfScore / len(kwords))\n",
    "\n",
    "                try:\n",
    "                    max_sorted = max(trIDFScores.items(), key=operator.itemgetter(1))\n",
    "                    keyword = max_sorted[0]\n",
    "                except:\n",
    "                    pass\n",
    "                return keyword\n",
    "        else:\n",
    "            try:\n",
    "                keyword = list(node_weight)[-1]\n",
    "            except:\n",
    "                pass\n",
    "            return keyword\n",
    "\n",
    "    # if return_one:\n",
    "    #     k_word = \"uncategorized\"\n",
    "    #     try:\n",
    "    #         max_sorted = max(trIDFScores.items(), key=operator.itemgetter(1))\n",
    "    #         k_word = max_sorted[0] #+ \" & \" + max_sorted[1]\n",
    "    #         # k_word = list(node_weight)[-1]\n",
    "    #     except:\n",
    "    #         pass\n",
    "    #     return k_word\n",
    "\n",
    "    return node_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello there are you okay', 'Are you sure you want to do this?')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_pairs([[\"hello there are you okay\", \"Are you sure you want to do this?\"], [\"hello there are you okay\", \"Are you sure you want to do this?\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mary (Python 3.7)",
   "language": "python",
   "name": "mary"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
